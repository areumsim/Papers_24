### Paged Attention, vLLM
 <br/>

## Title : Efficient Memory Management for Large Language Model Serving with PagedAttention   
> **Journal/Coference** : SOSP   
> **Year** : 2023   
> **Author** :  
> **Label / Tag** : LLM Serving, PagedAttention  
> **Url** :  https://arxiv.org/abs/2309.06180  / prj : https://docs.vllm.ai/en/latest/  
> **Github** :  https://github.com/vllm-project/vllm  

<br/>
<br/>

 
**About** :
- operating system에 사용되는 paging, virtual memory를 이용한 paged attention 제안
- LLM Inference 성능 향상

 <br/>

 